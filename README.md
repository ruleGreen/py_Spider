py_Spider
======

一个使用代理ip的简单爬虫，每一个线程使用一个代理ip进行爬取数据，并且在遇到封锁的情况下将当前
线程自动暂停一定的时间，随后从停止的位置开始接着爬，爬取的数据使用列表存储，并提供两种方式进
行本地存储。一是以txt格式进行输入，二是使用数据库操作类将爬取出来的数据进行存储在数据库中，
数据库选取Mysql,Oracle均可。

此文件夹需要注意的地方有以下几点:
>> 1.运行时需要先运行main文件，这样可以得到在getdata里面使用的数据<br/>
>> 2.其次getdata文件是分开运行的，由于我把main文件运行出来的数据直接存在getdata里面了，所以可以单独运行<br/>
>> 3.数据库连接采用pyodbc的方法，可自行百度操作，或者使用output方法输出为txt文档


getIp.py
----
本文件自动爬取第三方网站上的代理ip并进行验证，将验证通过的代理ip存储在checkedProxyList当中

使用的第三方网站：


                                * http://www.proxy360.cn/Region/Brazil
                                * http://www.proxy360.cn/Region/China
                                * http://www.proxy360.cn/Region/America
               

由于目标网站数目过多（防止数目太少爬取不到相应规模的代理ip），所以采用python的多线程处理，同时
开启20个线程进行爬取,再分别进行验证，最终得到可用的代理ip


html_downloader.py
-----
本文件主要通过urllib2来设置代理ip以及伪装浏览器等反反爬虫策略，代理ip是从前面检测出来的可以用的
ip中随机选取出来，并且对于可能出现的urlerror进行了异常处理


main.py
----
本文件主要通过浏览器渲染页面时的加载json文件爬取相应的事项编码，对于不同的部门来说事项在一个page页
无法全部显示，所以需要爬取json数据里面的totalpage一栏得知所有的page数目，根据所得的page数目迭代
爬取所有的事项编码


getData.py
----
这个文件很多坑，比如string方法和get_text方法或导致列表索引越界，希望仔细观看，这个文件里面主要介绍了
如何获取html源码里面不同的元素以及保存在列表中，还有就是处理一些错误的方法。


可加技术交流QQ群：475478017


